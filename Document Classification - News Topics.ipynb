{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from dalab import read_pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from langdetect import detect\n",
    "from collections import Counter\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "from time import time\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, Dense, Conv1D, MaxPooling1D, Flatten, Dropout, SimpleRNN, GRU, LSTM\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19394</th>\n",
       "      <td>misc</td>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu talk.religion....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7501</th>\n",
       "      <td>crypt</td>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu sci.crypt:1571...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19374</th>\n",
       "      <td>misc</td>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu alt.atheism:53...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14804</th>\n",
       "      <td>electronics</td>\n",
       "      <td>Newsgroups: sci.electronics\\nPath: cantaloupe....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11602</th>\n",
       "      <td>x</td>\n",
       "      <td>Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             label                                               text\n",
       "19394         misc  Xref: cantaloupe.srv.cs.cmu.edu talk.religion....\n",
       "7501         crypt  Xref: cantaloupe.srv.cs.cmu.edu sci.crypt:1571...\n",
       "19374         misc  Xref: cantaloupe.srv.cs.cmu.edu alt.atheism:53...\n",
       "14804  electronics  Newsgroups: sci.electronics\\nPath: cantaloupe....\n",
       "11602            x  Path: cantaloupe.srv.cs.cmu.edu!das-news.harva..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = read_pickle('data/20_newsgroup/dataframes/raw_news.pickle')\n",
    "df = df.sample(frac=1)\n",
    "df = df.drop_duplicates(subset='text')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXLEN = 1000\n",
    "VOCAB_SIZE = 20000\n",
    "TRAIN_SIZE = 15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing here:\n",
    "# Lower, remove unwanted chars, decide if is going to keep punctuations, lemmas, so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = word_tokenize(' '.join(df.text.tolist()))\n",
    "word_counts = Counter(all_words).most_common(VOCAB_SIZE)\n",
    "words = [w[0] for w in word_counts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "9000\n",
      "9500\n",
      "10000\n",
      "10500\n",
      "11000\n",
      "11500\n",
      "12000\n",
      "12500\n",
      "13000\n",
      "13500\n",
      "14000\n",
      "14500\n",
      "15000\n",
      "15500\n",
      "16000\n",
      "16500\n",
      "17000\n",
      "17500\n",
      "18000\n",
      "18500\n",
      "19000\n",
      "19500\n"
     ]
    }
   ],
   "source": [
    "embed_dic = {}\n",
    "for index, word in enumerate(words):\n",
    "    if index % 500 == 0: print(index)\n",
    "    token = nlp(word)\n",
    "    embed_dic[token.text] = token.vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>374</th>\n",
       "      <th>375</th>\n",
       "      <th>376</th>\n",
       "      <th>377</th>\n",
       "      <th>378</th>\n",
       "      <th>379</th>\n",
       "      <th>380</th>\n",
       "      <th>381</th>\n",
       "      <th>382</th>\n",
       "      <th>383</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>\u0002</th>\n",
       "      <td>-0.917688</td>\n",
       "      <td>1.594070</td>\n",
       "      <td>6.749008</td>\n",
       "      <td>0.536356</td>\n",
       "      <td>-1.675298</td>\n",
       "      <td>3.894897</td>\n",
       "      <td>-3.105777</td>\n",
       "      <td>2.663857</td>\n",
       "      <td>1.218641</td>\n",
       "      <td>-1.035807</td>\n",
       "      <td>...</td>\n",
       "      <td>0.425276</td>\n",
       "      <td>1.230748</td>\n",
       "      <td>0.118247</td>\n",
       "      <td>-0.219932</td>\n",
       "      <td>0.352065</td>\n",
       "      <td>-0.254422</td>\n",
       "      <td>-0.038656</td>\n",
       "      <td>-1.410054</td>\n",
       "      <td>0.657522</td>\n",
       "      <td>-0.292479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>!</th>\n",
       "      <td>0.671140</td>\n",
       "      <td>-0.475781</td>\n",
       "      <td>1.225882</td>\n",
       "      <td>-0.533356</td>\n",
       "      <td>1.413614</td>\n",
       "      <td>2.528172</td>\n",
       "      <td>-0.030113</td>\n",
       "      <td>0.486537</td>\n",
       "      <td>3.412096</td>\n",
       "      <td>1.299003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.361494</td>\n",
       "      <td>0.078374</td>\n",
       "      <td>-0.094767</td>\n",
       "      <td>-0.087820</td>\n",
       "      <td>-0.176552</td>\n",
       "      <td>0.149058</td>\n",
       "      <td>0.224980</td>\n",
       "      <td>-0.329079</td>\n",
       "      <td>0.187947</td>\n",
       "      <td>-0.189483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#</th>\n",
       "      <td>1.499199</td>\n",
       "      <td>-0.151666</td>\n",
       "      <td>2.150062</td>\n",
       "      <td>1.835209</td>\n",
       "      <td>1.904099</td>\n",
       "      <td>2.142193</td>\n",
       "      <td>-1.108657</td>\n",
       "      <td>-1.281631</td>\n",
       "      <td>2.732129</td>\n",
       "      <td>2.948512</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079698</td>\n",
       "      <td>-0.464941</td>\n",
       "      <td>1.290173</td>\n",
       "      <td>0.061074</td>\n",
       "      <td>-0.257399</td>\n",
       "      <td>-0.752442</td>\n",
       "      <td>0.019620</td>\n",
       "      <td>0.132082</td>\n",
       "      <td>-0.440150</td>\n",
       "      <td>-0.476223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>$</th>\n",
       "      <td>0.630779</td>\n",
       "      <td>1.138584</td>\n",
       "      <td>2.530838</td>\n",
       "      <td>0.166183</td>\n",
       "      <td>3.076835</td>\n",
       "      <td>0.542186</td>\n",
       "      <td>-0.858887</td>\n",
       "      <td>0.884039</td>\n",
       "      <td>2.754835</td>\n",
       "      <td>-0.390936</td>\n",
       "      <td>...</td>\n",
       "      <td>0.297402</td>\n",
       "      <td>-0.254304</td>\n",
       "      <td>1.426802</td>\n",
       "      <td>-0.010306</td>\n",
       "      <td>-0.657113</td>\n",
       "      <td>-0.627469</td>\n",
       "      <td>0.097199</td>\n",
       "      <td>-0.183204</td>\n",
       "      <td>-0.213610</td>\n",
       "      <td>-0.170229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>%</th>\n",
       "      <td>2.040906</td>\n",
       "      <td>0.173398</td>\n",
       "      <td>2.365521</td>\n",
       "      <td>-1.138491</td>\n",
       "      <td>0.034594</td>\n",
       "      <td>2.351219</td>\n",
       "      <td>-2.068765</td>\n",
       "      <td>-0.857941</td>\n",
       "      <td>0.967327</td>\n",
       "      <td>2.126300</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.527834</td>\n",
       "      <td>-0.229152</td>\n",
       "      <td>-0.059828</td>\n",
       "      <td>0.299519</td>\n",
       "      <td>-0.925737</td>\n",
       "      <td>-0.175775</td>\n",
       "      <td>0.280792</td>\n",
       "      <td>0.260768</td>\n",
       "      <td>0.674299</td>\n",
       "      <td>0.673200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&amp;</th>\n",
       "      <td>-0.362176</td>\n",
       "      <td>-1.536422</td>\n",
       "      <td>0.681592</td>\n",
       "      <td>-0.254282</td>\n",
       "      <td>-0.020795</td>\n",
       "      <td>2.549080</td>\n",
       "      <td>1.063519</td>\n",
       "      <td>1.306450</td>\n",
       "      <td>1.050382</td>\n",
       "      <td>2.485573</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.431890</td>\n",
       "      <td>-0.154748</td>\n",
       "      <td>-0.647066</td>\n",
       "      <td>-0.048509</td>\n",
       "      <td>0.023910</td>\n",
       "      <td>-0.560396</td>\n",
       "      <td>0.427393</td>\n",
       "      <td>0.642400</td>\n",
       "      <td>0.882393</td>\n",
       "      <td>-0.388471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'</th>\n",
       "      <td>-1.620776</td>\n",
       "      <td>2.052795</td>\n",
       "      <td>0.476201</td>\n",
       "      <td>-0.315580</td>\n",
       "      <td>0.532586</td>\n",
       "      <td>-0.451270</td>\n",
       "      <td>-1.238636</td>\n",
       "      <td>0.606207</td>\n",
       "      <td>-0.797014</td>\n",
       "      <td>0.126661</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054849</td>\n",
       "      <td>-0.057004</td>\n",
       "      <td>0.090347</td>\n",
       "      <td>-0.235629</td>\n",
       "      <td>-0.785747</td>\n",
       "      <td>-0.306805</td>\n",
       "      <td>0.576140</td>\n",
       "      <td>0.273453</td>\n",
       "      <td>0.878228</td>\n",
       "      <td>0.013317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>''</th>\n",
       "      <td>-1.888850</td>\n",
       "      <td>-0.329437</td>\n",
       "      <td>2.229829</td>\n",
       "      <td>-0.024572</td>\n",
       "      <td>0.612867</td>\n",
       "      <td>1.830826</td>\n",
       "      <td>-2.658098</td>\n",
       "      <td>1.066225</td>\n",
       "      <td>-0.894128</td>\n",
       "      <td>0.677597</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.093398</td>\n",
       "      <td>-0.030841</td>\n",
       "      <td>-0.173364</td>\n",
       "      <td>-0.138874</td>\n",
       "      <td>-0.683034</td>\n",
       "      <td>-0.094501</td>\n",
       "      <td>0.465635</td>\n",
       "      <td>0.371119</td>\n",
       "      <td>0.759352</td>\n",
       "      <td>0.149494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'*</th>\n",
       "      <td>1.111745</td>\n",
       "      <td>1.290676</td>\n",
       "      <td>-0.382141</td>\n",
       "      <td>0.069083</td>\n",
       "      <td>-0.239147</td>\n",
       "      <td>0.952505</td>\n",
       "      <td>-1.702639</td>\n",
       "      <td>1.894596</td>\n",
       "      <td>1.396065</td>\n",
       "      <td>0.785170</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027173</td>\n",
       "      <td>0.091146</td>\n",
       "      <td>-0.310856</td>\n",
       "      <td>-0.363376</td>\n",
       "      <td>-0.392699</td>\n",
       "      <td>0.008236</td>\n",
       "      <td>-0.072186</td>\n",
       "      <td>0.011815</td>\n",
       "      <td>0.666233</td>\n",
       "      <td>-0.253421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'+</th>\n",
       "      <td>0.697193</td>\n",
       "      <td>0.727430</td>\n",
       "      <td>0.795636</td>\n",
       "      <td>0.136264</td>\n",
       "      <td>0.777219</td>\n",
       "      <td>0.521605</td>\n",
       "      <td>-0.411750</td>\n",
       "      <td>0.238003</td>\n",
       "      <td>-0.009105</td>\n",
       "      <td>0.776705</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075038</td>\n",
       "      <td>0.048530</td>\n",
       "      <td>0.151208</td>\n",
       "      <td>-0.007558</td>\n",
       "      <td>-0.555188</td>\n",
       "      <td>-0.124780</td>\n",
       "      <td>0.504512</td>\n",
       "      <td>0.432797</td>\n",
       "      <td>1.108591</td>\n",
       "      <td>0.201013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 384 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6    \\\n",
       "\u0002  -0.917688  1.594070  6.749008  0.536356 -1.675298  3.894897 -3.105777   \n",
       "!   0.671140 -0.475781  1.225882 -0.533356  1.413614  2.528172 -0.030113   \n",
       "#   1.499199 -0.151666  2.150062  1.835209  1.904099  2.142193 -1.108657   \n",
       "$   0.630779  1.138584  2.530838  0.166183  3.076835  0.542186 -0.858887   \n",
       "%   2.040906  0.173398  2.365521 -1.138491  0.034594  2.351219 -2.068765   \n",
       "&  -0.362176 -1.536422  0.681592 -0.254282 -0.020795  2.549080  1.063519   \n",
       "'  -1.620776  2.052795  0.476201 -0.315580  0.532586 -0.451270 -1.238636   \n",
       "'' -1.888850 -0.329437  2.229829 -0.024572  0.612867  1.830826 -2.658098   \n",
       "'*  1.111745  1.290676 -0.382141  0.069083 -0.239147  0.952505 -1.702639   \n",
       "'+  0.697193  0.727430  0.795636  0.136264  0.777219  0.521605 -0.411750   \n",
       "\n",
       "         7         8         9      ...          374       375       376  \\\n",
       "\u0002   2.663857  1.218641 -1.035807    ...     0.425276  1.230748  0.118247   \n",
       "!   0.486537  3.412096  1.299003    ...     0.361494  0.078374 -0.094767   \n",
       "#  -1.281631  2.732129  2.948512    ...     0.079698 -0.464941  1.290173   \n",
       "$   0.884039  2.754835 -0.390936    ...     0.297402 -0.254304  1.426802   \n",
       "%  -0.857941  0.967327  2.126300    ...    -0.527834 -0.229152 -0.059828   \n",
       "&   1.306450  1.050382  2.485573    ...    -0.431890 -0.154748 -0.647066   \n",
       "'   0.606207 -0.797014  0.126661    ...     0.054849 -0.057004  0.090347   \n",
       "''  1.066225 -0.894128  0.677597    ...    -0.093398 -0.030841 -0.173364   \n",
       "'*  1.894596  1.396065  0.785170    ...    -0.027173  0.091146 -0.310856   \n",
       "'+  0.238003 -0.009105  0.776705    ...     0.075038  0.048530  0.151208   \n",
       "\n",
       "         377       378       379       380       381       382       383  \n",
       "\u0002  -0.219932  0.352065 -0.254422 -0.038656 -1.410054  0.657522 -0.292479  \n",
       "!  -0.087820 -0.176552  0.149058  0.224980 -0.329079  0.187947 -0.189483  \n",
       "#   0.061074 -0.257399 -0.752442  0.019620  0.132082 -0.440150 -0.476223  \n",
       "$  -0.010306 -0.657113 -0.627469  0.097199 -0.183204 -0.213610 -0.170229  \n",
       "%   0.299519 -0.925737 -0.175775  0.280792  0.260768  0.674299  0.673200  \n",
       "&  -0.048509  0.023910 -0.560396  0.427393  0.642400  0.882393 -0.388471  \n",
       "'  -0.235629 -0.785747 -0.306805  0.576140  0.273453  0.878228  0.013317  \n",
       "'' -0.138874 -0.683034 -0.094501  0.465635  0.371119  0.759352  0.149494  \n",
       "'* -0.363376 -0.392699  0.008236 -0.072186  0.011815  0.666233 -0.253421  \n",
       "'+ -0.007558 -0.555188 -0.124780  0.504512  0.432797  1.108591  0.201013  \n",
       "\n",
       "[10 rows x 384 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_words = pd.DataFrame(embed_dic).T\n",
    "embed_words.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>374</th>\n",
       "      <th>375</th>\n",
       "      <th>376</th>\n",
       "      <th>377</th>\n",
       "      <th>378</th>\n",
       "      <th>379</th>\n",
       "      <th>380</th>\n",
       "      <th>381</th>\n",
       "      <th>382</th>\n",
       "      <th>383</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&lt;PAD&gt;</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\u0002</th>\n",
       "      <td>-0.917688</td>\n",
       "      <td>1.594070</td>\n",
       "      <td>6.749008</td>\n",
       "      <td>0.536356</td>\n",
       "      <td>-1.675298</td>\n",
       "      <td>3.894897</td>\n",
       "      <td>-3.105777</td>\n",
       "      <td>2.663857</td>\n",
       "      <td>1.218641</td>\n",
       "      <td>-1.035807</td>\n",
       "      <td>...</td>\n",
       "      <td>0.425276</td>\n",
       "      <td>1.230748</td>\n",
       "      <td>0.118247</td>\n",
       "      <td>-0.219932</td>\n",
       "      <td>0.352065</td>\n",
       "      <td>-0.254422</td>\n",
       "      <td>-0.038656</td>\n",
       "      <td>-1.410054</td>\n",
       "      <td>0.657522</td>\n",
       "      <td>-0.292479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>!</th>\n",
       "      <td>0.671140</td>\n",
       "      <td>-0.475781</td>\n",
       "      <td>1.225882</td>\n",
       "      <td>-0.533356</td>\n",
       "      <td>1.413614</td>\n",
       "      <td>2.528172</td>\n",
       "      <td>-0.030113</td>\n",
       "      <td>0.486537</td>\n",
       "      <td>3.412096</td>\n",
       "      <td>1.299003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.361494</td>\n",
       "      <td>0.078374</td>\n",
       "      <td>-0.094767</td>\n",
       "      <td>-0.087820</td>\n",
       "      <td>-0.176552</td>\n",
       "      <td>0.149058</td>\n",
       "      <td>0.224980</td>\n",
       "      <td>-0.329079</td>\n",
       "      <td>0.187947</td>\n",
       "      <td>-0.189483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#</th>\n",
       "      <td>1.499199</td>\n",
       "      <td>-0.151666</td>\n",
       "      <td>2.150062</td>\n",
       "      <td>1.835209</td>\n",
       "      <td>1.904099</td>\n",
       "      <td>2.142193</td>\n",
       "      <td>-1.108657</td>\n",
       "      <td>-1.281631</td>\n",
       "      <td>2.732129</td>\n",
       "      <td>2.948512</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079698</td>\n",
       "      <td>-0.464941</td>\n",
       "      <td>1.290173</td>\n",
       "      <td>0.061074</td>\n",
       "      <td>-0.257399</td>\n",
       "      <td>-0.752442</td>\n",
       "      <td>0.019620</td>\n",
       "      <td>0.132082</td>\n",
       "      <td>-0.440150</td>\n",
       "      <td>-0.476223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>$</th>\n",
       "      <td>0.630779</td>\n",
       "      <td>1.138584</td>\n",
       "      <td>2.530838</td>\n",
       "      <td>0.166183</td>\n",
       "      <td>3.076835</td>\n",
       "      <td>0.542186</td>\n",
       "      <td>-0.858887</td>\n",
       "      <td>0.884039</td>\n",
       "      <td>2.754835</td>\n",
       "      <td>-0.390936</td>\n",
       "      <td>...</td>\n",
       "      <td>0.297402</td>\n",
       "      <td>-0.254304</td>\n",
       "      <td>1.426802</td>\n",
       "      <td>-0.010306</td>\n",
       "      <td>-0.657113</td>\n",
       "      <td>-0.627469</td>\n",
       "      <td>0.097199</td>\n",
       "      <td>-0.183204</td>\n",
       "      <td>-0.213610</td>\n",
       "      <td>-0.170229</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 384 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6    \\\n",
       "<PAD>  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\u0002     -0.917688  1.594070  6.749008  0.536356 -1.675298  3.894897 -3.105777   \n",
       "!      0.671140 -0.475781  1.225882 -0.533356  1.413614  2.528172 -0.030113   \n",
       "#      1.499199 -0.151666  2.150062  1.835209  1.904099  2.142193 -1.108657   \n",
       "$      0.630779  1.138584  2.530838  0.166183  3.076835  0.542186 -0.858887   \n",
       "\n",
       "            7         8         9      ...          374       375       376  \\\n",
       "<PAD>  0.000000  0.000000  0.000000    ...     0.000000  0.000000  0.000000   \n",
       "\u0002      2.663857  1.218641 -1.035807    ...     0.425276  1.230748  0.118247   \n",
       "!      0.486537  3.412096  1.299003    ...     0.361494  0.078374 -0.094767   \n",
       "#     -1.281631  2.732129  2.948512    ...     0.079698 -0.464941  1.290173   \n",
       "$      0.884039  2.754835 -0.390936    ...     0.297402 -0.254304  1.426802   \n",
       "\n",
       "            377       378       379       380       381       382       383  \n",
       "<PAD>  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "\u0002     -0.219932  0.352065 -0.254422 -0.038656 -1.410054  0.657522 -0.292479  \n",
       "!     -0.087820 -0.176552  0.149058  0.224980 -0.329079  0.187947 -0.189483  \n",
       "#      0.061074 -0.257399 -0.752442  0.019620  0.132082 -0.440150 -0.476223  \n",
       "$     -0.010306 -0.657113 -0.627469  0.097199 -0.183204 -0.213610 -0.170229  \n",
       "\n",
       "[5 rows x 384 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding = pd.DataFrame({'<PAD>': np.zeros(shape=[1,embed_words.shape[1]])[0]}).T\n",
    "embed_matrix = padding.append(embed_words)\n",
    "embed_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_matrix = np.random.randn(embed_matrix.shape[0], embed_matrix.shape[1])\n",
    "random_matrix[0] = np.zeros([1, embed_matrix.shape[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = {j:i+1 for i,j in enumerate(embed_matrix.index.tolist()[1:])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.word_index = word_index\n",
    "sequences = tokenizer.texts_to_sequences(df.text)\n",
    "data = pad_sequences(sequences, maxlen=MAXLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.     ],\n",
       "       [0.5307 ],\n",
       "       [0.92545],\n",
       "       [0.8407 ],\n",
       "       [0.7324 ],\n",
       "       [0.92545],\n",
       "       [0.80025],\n",
       "       [0.6359 ],\n",
       "       [0.534  ],\n",
       "       [0.6833 ],\n",
       "       [0.7555 ],\n",
       "       [0.92545],\n",
       "       [0.8407 ],\n",
       "       [0.7324 ],\n",
       "       [0.92545],\n",
       "       [0.80025],\n",
       "       [0.6359 ],\n",
       "       [0.534  ],\n",
       "       [0.6833 ],\n",
       "       [0.7856 ],\n",
       "       [0.5307 ],\n",
       "       [0.5307 ],\n",
       "       [0.5014 ],\n",
       "       [0.65315],\n",
       "       [0.7499 ],\n",
       "       [0.9058 ],\n",
       "       [0.65335],\n",
       "       [0.5014 ],\n",
       "       [0.7571 ],\n",
       "       [0.5014 ],\n",
       "       [0.99465],\n",
       "       [0.99465],\n",
       "       [0.6357 ],\n",
       "       [0.5508 ],\n",
       "       [0.5307 ],\n",
       "       [0.47385],\n",
       "       [0.75145],\n",
       "       [0.47385],\n",
       "       [0.82535],\n",
       "       [0.47385],\n",
       "       [0.735  ],\n",
       "       [0.6178 ],\n",
       "       [0.735  ],\n",
       "       [0.803  ],\n",
       "       [0.47385],\n",
       "       [0.70105],\n",
       "       [0.91385],\n",
       "       [0.83045],\n",
       "       [0.75635],\n",
       "       [0.42575],\n",
       "       [0.68335],\n",
       "       [0.7276 ],\n",
       "       [0.65715],\n",
       "       [0.82535],\n",
       "       [0.47385],\n",
       "       [0.8748 ],\n",
       "       [0.735  ],\n",
       "       [0.82535],\n",
       "       [0.47385],\n",
       "       [0.70105],\n",
       "       [0.77475],\n",
       "       [0.8551 ],\n",
       "       [0.7302 ],\n",
       "       [0.50055],\n",
       "       [0.44505],\n",
       "       [0.47365],\n",
       "       [0.83635],\n",
       "       [0.03595],\n",
       "       [0.05805],\n",
       "       [0.9639 ],\n",
       "       [0.7433 ],\n",
       "       [0.95445],\n",
       "       [0.7433 ],\n",
       "       [0.53905],\n",
       "       [0.039  ],\n",
       "       [0.0327 ],\n",
       "       [0.02515],\n",
       "       [0.04685],\n",
       "       [0.02175],\n",
       "       [0.70605],\n",
       "       [0.0641 ],\n",
       "       [0.6629 ],\n",
       "       [0.4355 ],\n",
       "       [0.7433 ],\n",
       "       [0.7986 ],\n",
       "       [0.79065],\n",
       "       [0.9927 ],\n",
       "       [0.6629 ],\n",
       "       [0.4355 ],\n",
       "       [0.95445],\n",
       "       [0.6025 ],\n",
       "       [0.95445],\n",
       "       [0.6162 ],\n",
       "       [0.9927 ],\n",
       "       [0.42895],\n",
       "       [0.7687 ],\n",
       "       [0.98955],\n",
       "       [0.9838 ],\n",
       "       [0.47245],\n",
       "       [0.6119 ],\n",
       "       [0.76755],\n",
       "       [0.9918 ],\n",
       "       [0.64145],\n",
       "       [0.9883 ],\n",
       "       [0.98195],\n",
       "       [0.55415],\n",
       "       [0.9303 ],\n",
       "       [0.64815],\n",
       "       [0.5578 ],\n",
       "       [0.9412 ],\n",
       "       [0.43395],\n",
       "       [0.93425],\n",
       "       [0.88255],\n",
       "       [0.45275],\n",
       "       [0.64815],\n",
       "       [0.6563 ],\n",
       "       [0.45395],\n",
       "       [0.9337 ],\n",
       "       [0.59045],\n",
       "       [0.82495],\n",
       "       [0.97955],\n",
       "       [0.42035],\n",
       "       [0.9337 ],\n",
       "       [0.70605],\n",
       "       [0.76755],\n",
       "       [0.65825],\n",
       "       [0.99595],\n",
       "       [0.64145],\n",
       "       [0.89565],\n",
       "       [0.87415],\n",
       "       [0.6807 ],\n",
       "       [0.42575],\n",
       "       [0.47295],\n",
       "       [0.40265],\n",
       "       [0.92255],\n",
       "       [0.9365 ],\n",
       "       [0.6178 ],\n",
       "       [0.40265],\n",
       "       [0.6362 ],\n",
       "       [0.98495],\n",
       "       [0.80535],\n",
       "       [0.71125],\n",
       "       [0.76755],\n",
       "       [0.8041 ],\n",
       "       [0.9412 ],\n",
       "       [0.6287 ],\n",
       "       [0.42575],\n",
       "       [0.76755],\n",
       "       [0.7045 ],\n",
       "       [0.6629 ],\n",
       "       [0.5915 ],\n",
       "       [0.61285],\n",
       "       [0.40265],\n",
       "       [0.7045 ],\n",
       "       [0.76755],\n",
       "       [0.9337 ],\n",
       "       [0.9006 ],\n",
       "       [0.65825],\n",
       "       [0.5541 ],\n",
       "       [0.6629 ],\n",
       "       [0.59825],\n",
       "       [0.50235],\n",
       "       [0.64815],\n",
       "       [0.5578 ],\n",
       "       [0.9412 ],\n",
       "       [0.43395],\n",
       "       [0.93435],\n",
       "       [0.6563 ],\n",
       "       [0.9918 ],\n",
       "       [0.9251 ],\n",
       "       [0.93355],\n",
       "       [0.4358 ],\n",
       "       [0.9956 ],\n",
       "       [0.4277 ],\n",
       "       [0.83225],\n",
       "       [0.9412 ],\n",
       "       [0.83895],\n",
       "       [0.4913 ],\n",
       "       [0.5631 ],\n",
       "       [0.61285],\n",
       "       [0.98385],\n",
       "       [0.68075],\n",
       "       [0.99155],\n",
       "       [0.70285],\n",
       "       [0.7235 ],\n",
       "       [0.4944 ],\n",
       "       [0.6629 ],\n",
       "       [0.77415],\n",
       "       [0.9412 ],\n",
       "       [0.73865],\n",
       "       [0.49555],\n",
       "       [0.95775],\n",
       "       [0.8697 ],\n",
       "       [0.7704 ],\n",
       "       [0.74375],\n",
       "       [0.71   ],\n",
       "       [0.6629 ],\n",
       "       [0.9337 ],\n",
       "       [0.5178 ],\n",
       "       [0.76755],\n",
       "       [0.98425],\n",
       "       [0.9337 ],\n",
       "       [0.9709 ],\n",
       "       [0.67935],\n",
       "       [0.6152 ],\n",
       "       [0.9337 ],\n",
       "       [0.8719 ],\n",
       "       [0.99595],\n",
       "       [0.83615],\n",
       "       [0.9412 ],\n",
       "       [0.64545],\n",
       "       [0.67935],\n",
       "       [0.6152 ],\n",
       "       [0.6629 ],\n",
       "       [0.03945],\n",
       "       [0.70515],\n",
       "       [0.9412 ],\n",
       "       [0.90535],\n",
       "       [0.446  ],\n",
       "       [0.4393 ],\n",
       "       [0.9709 ],\n",
       "       [0.0464 ],\n",
       "       [0.42575],\n",
       "       [0.66375],\n",
       "       [0.6807 ],\n",
       "       [0.64545],\n",
       "       [0.61285],\n",
       "       [0.9337 ],\n",
       "       [0.8622 ],\n",
       "       [0.76755],\n",
       "       [0.9344 ],\n",
       "       [0.43625],\n",
       "       [0.93425],\n",
       "       [0.98415],\n",
       "       [0.6563 ],\n",
       "       [0.87555],\n",
       "       [0.99595],\n",
       "       [0.98855],\n",
       "       [0.8214 ],\n",
       "       [0.4468 ],\n",
       "       [0.77345],\n",
       "       [0.5541 ],\n",
       "       [0.99595],\n",
       "       [0.6954 ],\n",
       "       [0.42885],\n",
       "       [0.7611 ],\n",
       "       [0.9337 ],\n",
       "       [0.5578 ],\n",
       "       [0.84385],\n",
       "       [0.9709 ],\n",
       "       [0.04665],\n",
       "       [0.8651 ],\n",
       "       [0.4727 ],\n",
       "       [0.762  ],\n",
       "       [0.65825],\n",
       "       [0.99595],\n",
       "       [0.64145],\n",
       "       [0.40265],\n",
       "       [0.8214 ],\n",
       "       [0.9251 ],\n",
       "       [0.6807 ],\n",
       "       [0.42575],\n",
       "       [0.42055],\n",
       "       [0.40265],\n",
       "       [0.4468 ],\n",
       "       [0.42575],\n",
       "       [0.65825],\n",
       "       [0.99595],\n",
       "       [0.5644 ],\n",
       "       [0.64145],\n",
       "       [0.40265],\n",
       "       [0.92255],\n",
       "       [0.87415],\n",
       "       [0.99625],\n",
       "       [0.42575],\n",
       "       [0.47295],\n",
       "       [0.7704 ],\n",
       "       [0.6807 ],\n",
       "       [0.67935],\n",
       "       [0.99285],\n",
       "       [0.42575],\n",
       "       [0.64195],\n",
       "       [0.97955],\n",
       "       [0.7631 ],\n",
       "       [0.9883 ],\n",
       "       [0.9337 ],\n",
       "       [0.9709 ],\n",
       "       [0.04685],\n",
       "       [0.8651 ],\n",
       "       [0.9337 ],\n",
       "       [0.5578 ],\n",
       "       [0.86195],\n",
       "       [0.8724 ],\n",
       "       [0.71035],\n",
       "       [0.64545],\n",
       "       [0.43265],\n",
       "       [0.95265],\n",
       "       [0.93355],\n",
       "       [0.67935],\n",
       "       [0.58385],\n",
       "       [0.64195],\n",
       "       [0.84385],\n",
       "       [0.9838 ],\n",
       "       [0.56345],\n",
       "       [0.9365 ],\n",
       "       [0.4192 ],\n",
       "       [0.7237 ],\n",
       "       [0.6629 ],\n",
       "       [0.9709 ],\n",
       "       [0.0464 ],\n",
       "       [0.7995 ],\n",
       "       [0.7763 ],\n",
       "       [0.9412 ],\n",
       "       [0.9337 ],\n",
       "       [0.5578 ],\n",
       "       [0.93355],\n",
       "       [0.98415],\n",
       "       [0.64195],\n",
       "       [0.87555],\n",
       "       [0.93425],\n",
       "       [0.7763 ],\n",
       "       [0.8089 ],\n",
       "       [0.93555],\n",
       "       [0.5541 ],\n",
       "       [0.7607 ],\n",
       "       [0.74835],\n",
       "       [0.9412 ],\n",
       "       [0.9251 ],\n",
       "       [0.42885],\n",
       "       [0.9883 ],\n",
       "       [0.93425],\n",
       "       [0.93395],\n",
       "       [0.6124 ],\n",
       "       [0.42575],\n",
       "       [0.9475 ],\n",
       "       [0.98315],\n",
       "       [0.7687 ],\n",
       "       [0.81785],\n",
       "       [0.93815],\n",
       "       [0.9337 ],\n",
       "       [0.76755],\n",
       "       [0.77595],\n",
       "       [0.65825],\n",
       "       [0.9709 ],\n",
       "       [0.04665],\n",
       "       [0.9412 ],\n",
       "       [0.9337 ],\n",
       "       [0.48675],\n",
       "       [0.6629 ],\n",
       "       [0.44145],\n",
       "       [0.93355],\n",
       "       [0.67935],\n",
       "       [0.9412 ],\n",
       "       [0.5018 ],\n",
       "       [0.6629 ],\n",
       "       [0.9337 ],\n",
       "       [0.7478 ],\n",
       "       [0.62075],\n",
       "       [0.84175],\n",
       "       [0.97955],\n",
       "       [0.86505],\n",
       "       [0.93535],\n",
       "       [0.93605],\n",
       "       [0.88255],\n",
       "       [0.45275],\n",
       "       [0.64195],\n",
       "       [0.97955],\n",
       "       [0.9412 ],\n",
       "       [0.45105],\n",
       "       [0.43485],\n",
       "       [0.6629 ],\n",
       "       [0.9709 ],\n",
       "       [0.04665],\n",
       "       [0.64195],\n",
       "       [0.9304 ],\n",
       "       [0.9337 ],\n",
       "       [0.5578 ],\n",
       "       [0.93355],\n",
       "       [0.93555],\n",
       "       [0.98605],\n",
       "       [0.64145],\n",
       "       [0.9412 ],\n",
       "       [0.9251 ],\n",
       "       [0.93395],\n",
       "       [0.77845],\n",
       "       [0.80275],\n",
       "       [0.42575],\n",
       "       [0.81835],\n",
       "       [0.9883 ],\n",
       "       [0.93425],\n",
       "       [0.93555],\n",
       "       [0.98605],\n",
       "       [0.64145],\n",
       "       [0.9412 ],\n",
       "       [0.4795 ],\n",
       "       [0.93395],\n",
       "       [0.77845],\n",
       "       [0.8214 ],\n",
       "       [0.452  ],\n",
       "       [0.935  ],\n",
       "       [0.98605],\n",
       "       [0.45105],\n",
       "       [0.7585 ],\n",
       "       [0.70985],\n",
       "       [0.4286 ],\n",
       "       [0.6178 ],\n",
       "       [0.4288 ],\n",
       "       [0.42575],\n",
       "       [0.935  ],\n",
       "       [0.98605],\n",
       "       [0.45105],\n",
       "       [0.94   ],\n",
       "       [0.76755],\n",
       "       [0.98415],\n",
       "       [0.93555],\n",
       "       [0.98605],\n",
       "       [0.64145],\n",
       "       [0.9412 ],\n",
       "       [0.9251 ],\n",
       "       [0.47855],\n",
       "       [0.76755],\n",
       "       [0.93435],\n",
       "       [0.9365 ],\n",
       "       [0.97955],\n",
       "       [0.6616 ],\n",
       "       [0.47335],\n",
       "       [0.9337 ],\n",
       "       [0.83625],\n",
       "       [0.9412 ],\n",
       "       [0.9337 ],\n",
       "       [0.92255],\n",
       "       [0.69315],\n",
       "       [0.93355],\n",
       "       [0.64195],\n",
       "       [0.97955],\n",
       "       [0.62895],\n",
       "       [0.9412 ],\n",
       "       [0.45105],\n",
       "       [0.43485],\n",
       "       [0.42575],\n",
       "       [0.6951 ],\n",
       "       [0.4358 ],\n",
       "       [0.40265],\n",
       "       [0.52855],\n",
       "       [0.9337 ],\n",
       "       [0.48785],\n",
       "       [0.46895],\n",
       "       [0.41515],\n",
       "       [0.6476 ],\n",
       "       [0.98315],\n",
       "       [0.7053 ],\n",
       "       [0.9412 ],\n",
       "       [0.45105],\n",
       "       [0.7845 ],\n",
       "       [0.77015],\n",
       "       [0.9412 ],\n",
       "       [0.64815],\n",
       "       [0.5578 ],\n",
       "       [0.6629 ],\n",
       "       [0.9709 ],\n",
       "       [0.04685],\n",
       "       [0.98145],\n",
       "       [0.8724 ],\n",
       "       [0.93355],\n",
       "       [0.9337 ],\n",
       "       [0.5578 ],\n",
       "       [0.64145],\n",
       "       [0.92515],\n",
       "       [0.94265],\n",
       "       [0.70725],\n",
       "       [0.6807 ],\n",
       "       [0.67935],\n",
       "       [0.7607 ],\n",
       "       [0.9337 ],\n",
       "       [0.60785],\n",
       "       [0.9399 ],\n",
       "       [0.93355],\n",
       "       [0.9337 ],\n",
       "       [0.5578 ],\n",
       "       [0.62975],\n",
       "       [0.64815],\n",
       "       [0.7276 ],\n",
       "       [0.99295],\n",
       "       [0.93555],\n",
       "       [0.6371 ],\n",
       "       [0.5647 ],\n",
       "       [0.9365 ],\n",
       "       [0.77015],\n",
       "       [0.8781 ],\n",
       "       [0.67195],\n",
       "       [0.76755],\n",
       "       [0.95785],\n",
       "       [0.9337 ],\n",
       "       [0.76755],\n",
       "       [0.64815],\n",
       "       [0.99015],\n",
       "       [0.93555],\n",
       "       [0.8075 ],\n",
       "       [0.6476 ],\n",
       "       [0.9883 ],\n",
       "       [0.95265],\n",
       "       [0.97955],\n",
       "       [0.78875],\n",
       "       [0.92515],\n",
       "       [0.47335],\n",
       "       [0.9365 ],\n",
       "       [0.81495],\n",
       "       [0.5111 ],\n",
       "       [0.9337 ],\n",
       "       [0.5591 ],\n",
       "       [0.47335],\n",
       "       [0.86505],\n",
       "       [0.93355],\n",
       "       [0.67935],\n",
       "       [0.58385],\n",
       "       [0.9337 ],\n",
       "       [0.5578 ],\n",
       "       [0.52415],\n",
       "       [0.7607 ],\n",
       "       [0.95775],\n",
       "       [0.98535],\n",
       "       [0.6371 ],\n",
       "       [0.9412 ],\n",
       "       [0.5542 ],\n",
       "       [0.68975],\n",
       "       [0.93035],\n",
       "       [0.93425],\n",
       "       [0.7775 ],\n",
       "       [0.42575],\n",
       "       [0.7775 ],\n",
       "       [0.4151 ],\n",
       "       [0.93355],\n",
       "       [0.64195],\n",
       "       [0.9918 ],\n",
       "       [0.89605],\n",
       "       [0.7607 ],\n",
       "       [0.45105],\n",
       "       [0.9883 ],\n",
       "       [0.93425],\n",
       "       [0.4727 ],\n",
       "       [0.93555],\n",
       "       [0.52415],\n",
       "       [0.7607 ],\n",
       "       [0.63195],\n",
       "       [0.9365 ],\n",
       "       [0.93555],\n",
       "       [0.97835],\n",
       "       [0.93395],\n",
       "       [0.7278 ],\n",
       "       [0.9412 ],\n",
       "       [0.45105],\n",
       "       [0.40265],\n",
       "       [0.80005],\n",
       "       [0.6997 ],\n",
       "       [0.98495],\n",
       "       [0.9918 ],\n",
       "       [0.69965],\n",
       "       [0.93425],\n",
       "       [0.6629 ],\n",
       "       [0.41515],\n",
       "       [0.9337 ],\n",
       "       [0.93355],\n",
       "       [0.67935],\n",
       "       [0.98535],\n",
       "       [0.93555],\n",
       "       [0.98315],\n",
       "       [0.8923 ],\n",
       "       [0.6892 ],\n",
       "       [0.77015],\n",
       "       [0.80755],\n",
       "       [0.6476 ],\n",
       "       [0.9883 ],\n",
       "       [0.9337 ],\n",
       "       [0.93555],\n",
       "       [0.6371 ],\n",
       "       [0.7585 ],\n",
       "       [0.6573 ],\n",
       "       [0.93355],\n",
       "       [0.97955],\n",
       "       [0.8365 ],\n",
       "       [0.9412 ],\n",
       "       [0.40265],\n",
       "       [0.5548 ],\n",
       "       [0.69215],\n",
       "       [0.76755],\n",
       "       [0.9845 ],\n",
       "       [0.93555],\n",
       "       [0.98315],\n",
       "       [0.9412 ],\n",
       "       [0.45105],\n",
       "       [0.40265],\n",
       "       [0.7832 ],\n",
       "       [0.76755],\n",
       "       [0.9337 ],\n",
       "       [0.5578 ],\n",
       "       [0.52415],\n",
       "       [0.77085],\n",
       "       [0.78805],\n",
       "       [0.76755],\n",
       "       [0.40265],\n",
       "       [0.69215],\n",
       "       [0.77015],\n",
       "       [0.57225],\n",
       "       [0.82905],\n",
       "       [0.9333 ],\n",
       "       [0.9337 ],\n",
       "       [0.7704 ],\n",
       "       [0.6629 ],\n",
       "       [0.6436 ],\n",
       "       [0.95775],\n",
       "       [0.93355],\n",
       "       [0.751  ],\n",
       "       [0.6738 ],\n",
       "       [0.64815],\n",
       "       [0.6119 ],\n",
       "       [0.9412 ],\n",
       "       [0.9251 ],\n",
       "       [0.9618 ],\n",
       "       [0.4343 ],\n",
       "       [0.6629 ],\n",
       "       [0.9365 ],\n",
       "       [0.97955],\n",
       "       [0.49555],\n",
       "       [0.6629 ],\n",
       "       [0.59   ],\n",
       "       [0.6629 ],\n",
       "       [0.03945],\n",
       "       [0.0533 ],\n",
       "       [0.98415],\n",
       "       [0.64815],\n",
       "       [0.5578 ],\n",
       "       [0.61285],\n",
       "       [0.96455],\n",
       "       [0.9337 ],\n",
       "       [0.92255],\n",
       "       [0.41515],\n",
       "       [0.9337 ],\n",
       "       [0.8767 ],\n",
       "       [0.76755],\n",
       "       [0.9337 ],\n",
       "       [0.64695],\n",
       "       [0.80925],\n",
       "       [0.98415],\n",
       "       [0.93555],\n",
       "       [0.47605],\n",
       "       [0.9412 ],\n",
       "       [0.4348 ],\n",
       "       [0.6476 ],\n",
       "       [0.4358 ],\n",
       "       [0.6206 ],\n",
       "       [0.59   ],\n",
       "       [0.6289 ],\n",
       "       [0.8923 ],\n",
       "       [0.6008 ],\n",
       "       [0.4358 ],\n",
       "       [0.9412 ],\n",
       "       [0.6427 ],\n",
       "       [0.9337 ],\n",
       "       [0.71665],\n",
       "       [0.6697 ],\n",
       "       [0.47335],\n",
       "       [0.9337 ],\n",
       "       [0.5578 ],\n",
       "       [0.5541 ],\n",
       "       [0.7607 ],\n",
       "       [0.5118 ],\n",
       "       [0.9734 ],\n",
       "       [0.4358 ],\n",
       "       [0.40265],\n",
       "       [0.72395],\n",
       "       [0.61285],\n",
       "       [0.64815],\n",
       "       [0.8213 ],\n",
       "       [0.5189 ],\n",
       "       [0.9365 ],\n",
       "       [0.45345],\n",
       "       [0.9883 ],\n",
       "       [0.93355],\n",
       "       [0.76755],\n",
       "       [0.5394 ],\n",
       "       [0.42575],\n",
       "       [0.99595],\n",
       "       [0.98605],\n",
       "       [0.49555],\n",
       "       [0.8724 ],\n",
       "       [0.93355],\n",
       "       [0.97955],\n",
       "       [0.7607 ],\n",
       "       [0.4358 ],\n",
       "       [0.64195],\n",
       "       [0.4942 ],\n",
       "       [0.9204 ],\n",
       "       [0.64765],\n",
       "       [0.9412 ],\n",
       "       [0.9337 ],\n",
       "       [0.42575],\n",
       "       [0.42   ],\n",
       "       [0.64765],\n",
       "       [0.9412 ],\n",
       "       [0.45105],\n",
       "       [0.52975],\n",
       "       [0.64195],\n",
       "       [0.55415],\n",
       "       [0.6283 ],\n",
       "       [0.7763 ],\n",
       "       [0.6629 ],\n",
       "       [0.40265],\n",
       "       [0.4602 ],\n",
       "       [0.76755],\n",
       "       [0.6277 ],\n",
       "       [0.4727 ],\n",
       "       [0.82905],\n",
       "       [0.97955],\n",
       "       [0.59235],\n",
       "       [0.70515],\n",
       "       [0.40265],\n",
       "       [0.5036 ],\n",
       "       [0.52855],\n",
       "       [0.6563 ],\n",
       "       [0.6508 ],\n",
       "       [0.9365 ],\n",
       "       [0.71615],\n",
       "       [0.6807 ],\n",
       "       [0.40265],\n",
       "       [0.7074 ],\n",
       "       [0.73865],\n",
       "       [0.4953 ],\n",
       "       [0.6287 ],\n",
       "       [0.45105],\n",
       "       [0.9883 ],\n",
       "       [0.99595],\n",
       "       [0.70105],\n",
       "       [0.98285],\n",
       "       [0.98415],\n",
       "       [0.9337 ],\n",
       "       [0.75675],\n",
       "       [0.60295],\n",
       "       [0.7684 ],\n",
       "       [0.5018 ],\n",
       "       [0.9412 ],\n",
       "       [0.74425],\n",
       "       [0.65275],\n",
       "       [0.9412 ],\n",
       "       [0.48905],\n",
       "       [0.7763 ],\n",
       "       [0.74425],\n",
       "       [0.5967 ],\n",
       "       [0.98195],\n",
       "       [0.4743 ],\n",
       "       [0.68795],\n",
       "       [0.45105],\n",
       "       [0.9196 ],\n",
       "       [0.7607 ],\n",
       "       [0.9412 ],\n",
       "       [0.8821 ],\n",
       "       [0.4393 ],\n",
       "       [0.93425],\n",
       "       [0.9282 ],\n",
       "       [0.6956 ],\n",
       "       [0.42575],\n",
       "       [0.6252 ],\n",
       "       [0.4098 ],\n",
       "       [0.9336 ],\n",
       "       [0.80835],\n",
       "       [0.7423 ],\n",
       "       [0.9337 ],\n",
       "       [0.4668 ],\n",
       "       [0.98415],\n",
       "       [0.9337 ],\n",
       "       [0.7086 ],\n",
       "       [0.69335],\n",
       "       [0.77015],\n",
       "       [0.93395],\n",
       "       [0.56485],\n",
       "       [0.76755],\n",
       "       [0.5252 ],\n",
       "       [0.98415],\n",
       "       [0.434  ],\n",
       "       [0.72645],\n",
       "       [0.93425],\n",
       "       [0.98855],\n",
       "       [0.6807 ],\n",
       "       [0.97955],\n",
       "       [0.40265],\n",
       "       [0.5548 ],\n",
       "       [0.7217 ],\n",
       "       [0.98535],\n",
       "       [0.5541 ],\n",
       "       [0.93555],\n",
       "       [0.7607 ],\n",
       "       [0.69345],\n",
       "       [0.93355],\n",
       "       [0.93535],\n",
       "       [0.72645],\n",
       "       [0.98315],\n",
       "       [0.60295],\n",
       "       [0.7684 ],\n",
       "       [0.98535],\n",
       "       [0.97955],\n",
       "       [0.935  ],\n",
       "       [0.40265],\n",
       "       [0.74835],\n",
       "       [0.61285],\n",
       "       [0.40265],\n",
       "       [0.61205],\n",
       "       [0.9742 ],\n",
       "       [0.4393 ],\n",
       "       [0.4192 ],\n",
       "       [0.55415],\n",
       "       [0.64145],\n",
       "       [0.40265],\n",
       "       [0.60455],\n",
       "       [0.99015],\n",
       "       [0.4045 ],\n",
       "       [0.91415],\n",
       "       [0.9412 ],\n",
       "       [0.6303 ],\n",
       "       [0.44275],\n",
       "       [0.7907 ],\n",
       "       [0.7133 ],\n",
       "       [0.9774 ],\n",
       "       [0.9337 ],\n",
       "       [0.8887 ],\n",
       "       [0.67935],\n",
       "       [0.70485],\n",
       "       [0.40565],\n",
       "       [0.51   ],\n",
       "       [0.48475],\n",
       "       [0.70285],\n",
       "       [0.9633 ],\n",
       "       [0.9412 ],\n",
       "       [0.9337 ],\n",
       "       [0.7499 ],\n",
       "       [0.4151 ],\n",
       "       [0.95955],\n",
       "       [0.76755],\n",
       "       [0.69015],\n",
       "       [0.7986 ],\n",
       "       [0.4148 ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.reshape(-1, data.shape[1], 1)\n",
    "data = data/VOCAB_SIZE\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot = pd.get_dummies(df['label'])\n",
    "target_labels = onehot.columns\n",
    "target = onehot.as_matrix()\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data[:TRAIN_SIZE]\n",
    "x_test = data[TRAIN_SIZE:]\n",
    "\n",
    "y_train = target[:TRAIN_SIZE]\n",
    "y_test = target[TRAIN_SIZE:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_shape=data.shape[1:]))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(35))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(target.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1000, 128)         256       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1000, 128)         16512     \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 996, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 199, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 195, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 39, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 39, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 35, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 1, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 17)                2193      \n",
      "=================================================================\n",
      "Total params: 281,617\n",
      "Trainable params: 281,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 4393 samples\n",
      "Epoch 1/2\n",
      "11264/15000 [=====================>........] - ETA: 81s - loss: 2.7783 - acc: 0.1401"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-368cdf934484>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    868\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 870\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1505\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1154\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1156\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1157\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2267\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2268\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2269\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2270\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=2, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-891cdcbaae37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0membedded_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConv1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded_sequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMaxPooling1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConv1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    613\u001b[0m             \u001b[0;31m# Infering the output shape is only relevant for Theano.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_to_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m                 \u001b[0moutput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mcompute_output_shape\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    189\u001b[0m                 new_dim = conv_utils.conv_output_length(\n\u001b[1;32m    190\u001b[0m                     \u001b[0mspace\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m                     \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m                     \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "embedding_layer = Embedding(len(embed_matrix), len(embed_matrix.columns), weights=[random_matrix],\n",
    "                            input_length=data.shape[1:], trainable=False)\n",
    "\n",
    "sequence_input = Input(shape=(MAXLEN,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(35)(x)  # global max pooling\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "output = Dense(target.shape[1], activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=2, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=2, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, activation='relu', input_shape=data.shape[1:], return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(128, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(target.shape[1], activation='softmax'))\n",
    "\n",
    "opt = keras.optimizers.Adam(lr=1e-3, decay=1e-5)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=2, batch_size=128)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
