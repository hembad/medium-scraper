{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import argparse\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.probability import FreqDist\n",
    "from heapq import nlargest\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "import glob\n",
    "from dalab import read_pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files = glob.glob('data/medium_stories/*.pickle')\n",
    "\n",
    "nlp = read_pickle('data/medium_stories/nlp.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "stories = ' '.join(nlp)\n",
    "stop_words = set(stopwords.words('english') + list(punctuation))\n",
    "words = [word for word in word_tokenize(stories.lower()) if word not in stop_words]\n",
    "word_freq = pd.Series(FreqDist(words))\n",
    "\n",
    "# Lots of wrong punctuation passing, ex: \\n, -, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(story, word_freq, n_sentences):\n",
    "    sentences = sent_tokenize(story)\n",
    "    ranking = defaultdict(float)\n",
    "    for index, sentence in enumerate(sentences):\n",
    "        for word in word_tokenize(sentence.lower()):\n",
    "            if word in word_freq:\n",
    "                ranking[sentence] += word_freq[word]\n",
    "\n",
    "    rank = pd.Series(ranking)\n",
    "    summary = ' '.join(rank.nlargest(n_sentences).index.tolist())\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tf-idf not only counts the occurrence of a word in the given text(or document), but also reflect how important the word is to the document. (Note that there is a typo in the second equation by the author, it should be P(a | Not Sports)\\xa0… )\\xa0\\xa0\\xa0Finally, here are some advanced techniques for improving the basic Naive Bayes classifier: 3.Comments\\xa0\\xa0When applying multinomial Naive Bayes to text classification problems, two questions that should be considered before getting started:\\xa0\\xa0(1) Which features of text are you going to extract? In his blog post “A practical explanation of a Naive Bayes classifier”, Bruno Stecanella, he walked us through an example, building a multinomial Naive Bayes classifier to solve a typical NLP problem: text classification. And you will find out that Naive Bayes classifiers are a good example of being both simple (naive) and powerful for NLP tasks such as text classification. Step 2: Being naive\\xa0\\xa0In the non-naive Bayes way, we look at sentences in entirety, thus once the sentence does not show up in the training set, we will get a zero probability, making it difficult for further calculations.'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize(nlp[50], word_freq, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
